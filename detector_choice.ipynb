{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loose-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-livestock",
   "metadata": {},
   "source": [
    "## Ground truth - annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "loose-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relative-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"dataset/second_approach/images/\"\n",
    "annot_dir = \"dataset/second_approach/annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "native-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = [file for file in glob.glob(annot_dir + \"*.xml\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-familiar",
   "metadata": {},
   "source": [
    "### Let's see exemplary files content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "surprising-member",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/second_approach/annotations/maksssksksss110.xml',\n",
       " 'dataset/second_approach/annotations/maksssksksss380.xml']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot1 = [annot[3]]+[annot[1]]\n",
    "annot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces(annotations, return_bdbox=False):\n",
    "    images = []\n",
    "    labels = []\n",
    "    files = []\n",
    "    for img_annot in annotations:\n",
    "        tree = ET.parse(img_annot)\n",
    "        root = tree.getroot()\n",
    "        file = root.find('filename').text\n",
    "        file_path = img_dir + file\n",
    "        img = cv.imread(file_path)\n",
    "        size = root.find('size')\n",
    "        width = int(size.find('width').text)\n",
    "        height = int(size.find('height').text)\n",
    "        x = img.shape[1]/width\n",
    "        y = img.shape[0]/height\n",
    "        objects = root.findall('object')\n",
    "        for object in objects:\n",
    "            label = object.find('name').text\n",
    "            place = str(len(objects)) + \": \" + label\n",
    "            labels.append(place)\n",
    "            bndbox = object.find('bndbox')\n",
    "            xmin = int((int(bndbox.find('xmin').text))/x)\n",
    "            ymin = int((int(bndbox.find('ymin').text))/y)\n",
    "            xmax = int((int(bndbox.find('xmax').text))/x)\n",
    "            ymax = int((int(bndbox.find('ymax').text))/y)\n",
    "            if return_bdbox:\n",
    "                images.append([xmin, ymin, xmax, ymax])\n",
    "                files.append(file_path)\n",
    "            else:\n",
    "                face = img[ymin:ymax, xmin:xmax]\n",
    "                images.append(face)\n",
    "    if return_bdbox:\n",
    "        return images, labels, files\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adolescent-washington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images1, labels1 = extract_faces(annot1)\n",
    "len(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 10))\n",
    "axes = []\n",
    "for i in range(len(images1)):\n",
    "    img = cv.cvtColor(images1[i], cv.COLOR_BGR2RGB)\n",
    "    axes.append(fig.add_subplot(3, 9, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.title(labels1[i])\n",
    "    plt.imshow(img)   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-doubt",
   "metadata": {},
   "source": [
    "What's good:\n",
    "- multiple faces at one photo\n",
    "- ocluded elements\n",
    "- diffetent positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-athens",
   "metadata": {},
   "source": [
    "## Let's take a closer look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cardiovascular-bookmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny face:  maksssksksss64.png size:  6\n",
      "Tiny face:  maksssksksss64.png size:  9\n",
      "Tiny face:  maksssksksss64.png size:  2\n",
      "Tiny face:  maksssksksss64.png size:  4\n",
      "Tiny face:  maksssksksss64.png size:  6\n",
      "Tiny face:  maksssksksss64.png size:  2\n",
      "Tiny face:  maksssksksss64.png size:  2\n",
      "Tiny face:  maksssksksss486.png size:  12\n",
      "Tiny face:  maksssksksss603.png size:  15\n"
     ]
    }
   ],
   "source": [
    "faces_on_photo = []\n",
    "photo_size = []\n",
    "\n",
    "face_sizes_area = []\n",
    "face_sizes_perc = []\n",
    "\n",
    "tiny_faces_annot = []\n",
    "tiny_faces_img = []\n",
    "tiny_faces = {}\n",
    "\n",
    "for img_annot in annot:\n",
    "    tree = ET.parse(img_annot)\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall('object')\n",
    "    faces_number = len(objects)\n",
    "    file = root.find('filename').text\n",
    "    file_path = img_dir + file\n",
    "    img = cv.imread(file_path)\n",
    "    img_size = img.shape[0]*img.shape[1]\n",
    "    photo_size.append(img_size**0.5)\n",
    "    size = root.find('size')\n",
    "    width = int(size.find('width').text)\n",
    "    height = int(size.find('height').text)\n",
    "    x = img.shape[1]/width\n",
    "    y = img.shape[0]/height\n",
    "    for object in objects:\n",
    "        bndbox = object.find('bndbox')\n",
    "        xmin = int((int(bndbox.find('xmin').text))/x)\n",
    "        ymin = int((int(bndbox.find('ymin').text))/y)\n",
    "        xmax = int((int(bndbox.find('xmax').text))/x)\n",
    "        ymax = int((int(bndbox.find('ymax').text))/y)\n",
    "        area = (xmax-xmin)*(ymax-ymin)\n",
    "        if(area < 16):\n",
    "            print(\"Tiny face: \", file, \"size: \", area)\n",
    "            tiny_faces_annot.append(img_annot)\n",
    "            tiny_faces_img.append(file_path)\n",
    "            faces_number -= 1\n",
    "            if (file_path in tiny_faces):\n",
    "                tiny_faces[file_path] += 1\n",
    "            else:\n",
    "                tiny_faces[file_path] = 1\n",
    "        else:\n",
    "            face_sizes_area.append(area**(0.5))\n",
    "            face_sizes_perc.append(100*area/img_size)\n",
    "    faces_on_photo.append(faces_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-surgeon",
   "metadata": {},
   "source": [
    "### Tiny faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "induced-remains",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/second_approach/annotations/maksssksksss603.xml',\n",
       " 'dataset/second_approach/annotations/maksssksksss64.xml',\n",
       " 'dataset/second_approach/annotations/maksssksksss486.xml']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_faces_annot = list(set(tiny_faces_annot))\n",
    "tiny_faces_img = list(set(tiny_faces_img))\n",
    "tiny_faces_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cleared-amino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/second_approach/images/maksssksksss603.png',\n",
       " 'dataset/second_approach/images/maksssksksss486.png',\n",
       " 'dataset/second_approach/images/maksssksksss64.png']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_faces_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "sublime-revolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/second_approach/images/maksssksksss603.png',\n",
       " 'dataset/second_approach/images/maksssksksss64.png',\n",
       " 'dataset/second_approach/images/maksssksksss486.png']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_faces_img = [tiny_faces_img[i] for i in [0, 2, 1]]\n",
    "tiny_faces_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ethical-meaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs2 = []\n",
    "for i in tiny_faces_img:\n",
    "    imgs2.append(cv.imread(i))\n",
    "images2, labels2, files2 = extract_faces(tiny_faces_annot, return_bdbox=True)\n",
    "len(labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "double-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "saving-onion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'dataset/second_approach/images/maksssksksss603.png': 115,\n",
       "         'dataset/second_approach/images/maksssksksss64.png': 11,\n",
       "         'dataset/second_approach/images/maksssksksss486.png': 8})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(files2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 10))\n",
    "axes = []\n",
    "for i in range(len(imgs2)):\n",
    "    img = cv.cvtColor(imgs2[i], cv.COLOR_BGR2RGB)\n",
    "    axes.append(fig.add_subplot(1,3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf146c8-3463-4ec2-86cc-7d6cd970386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 30))\n",
    "\n",
    "for i in range(len(images2)):\n",
    "    bdbox = images2[i]\n",
    "    if ((bdbox[2]-bdbox[0])*(bdbox[3]-bdbox[1]) < 16):\n",
    "        idx = tiny_faces_img.index(files2[i])\n",
    "        cv.rectangle(imgs2[idx], (bdbox[0], bdbox[1]), (bdbox[2], bdbox[3]), (0, 0, 255), 2)\n",
    "for i in range(len(imgs2)):\n",
    "    img = cv.cvtColor(imgs2[i], cv.COLOR_BGR2RGB)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-belfast",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "recreational-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_size = [x / 10**3 for x in photo_size]\n",
    "photo_characteristics = pd.DataFrame({'Faces on photo': faces_on_photo, 'Photo size (kilopixels)': photo_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09beccb0-d1bd-444c-9a3c-d0f91d38c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(photo_characteristics, x='Photo size (kilopixels)', bins=int((len(photo_characteristics))**(0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.displot(photo_characteristics, x='Faces on photo', bins=int((len(photo_characteristics))**(0.5)))\n",
    "for ax in fig.axes.flat:\n",
    "    ax.set_yscale('log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "regulation-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_characteristics = pd.DataFrame({'Faces area': face_sizes_area, 'Faces percentage': face_sizes_perc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b05d0-7b56-47e1-acb1-29c2501bcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(faces_characteristics, x='Faces area', bins=int((len(faces_characteristics))**(0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "original-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as tkr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.displot(faces_characteristics, x='Faces percentage', bins=int((len(faces_characteristics))**(0.5)))\n",
    "for ax in fig.axes.flat:\n",
    "    ax.set_yscale('log')\n",
    "    ax.xaxis.set_major_formatter(tkr.FuncFormatter(lambda x, p: f'{x: .0f}%'))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-cleaner",
   "metadata": {},
   "source": [
    "### Exemplary files - detection check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advisory-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = 'dataset/second_approach/images/'\n",
    "exemplary_annot = ['dataset/second_approach/annotations/maksssksksss521.xml',\n",
    "                  'dataset/second_approach/annotations/maksssksksss473.xml',\n",
    "                  'dataset/second_approach/annotations/maksssksksss110.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "korean-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplary_imgs_files = ['dataset/second_approach/images/maksssksksss521.png',\n",
    "                        'dataset/second_approach/images/maksssksksss473.png',\n",
    "                        'dataset/second_approach/images/maksssksksss110.png']\n",
    "exemplary_imgs = [cv.imread(file) for file in exemplary_imgs_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "racial-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplary_images, exemplary_labels, exemplary_files = extract_faces(exemplary_annot, return_bdbox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "\n",
    "for i in range(len(exemplary_images)):\n",
    "    bdbox = exemplary_images[i]\n",
    "    idx = exemplary_imgs_files.index(exemplary_files[i])\n",
    "    cv.rectangle(exemplary_imgs[idx], (bdbox[0], bdbox[1]), (bdbox[2], bdbox[3]), (0, 0, 255), 1)\n",
    "for i in range(len(exemplary_imgs)):\n",
    "    img = cv.cvtColor(exemplary_imgs[i], cv.COLOR_BGR2RGB)\n",
    "    axes.append(fig.add_subplot(1,3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3540a-8ace-46f9-b090-41adb4e6906a",
   "metadata": {},
   "source": [
    "## Annotation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc5ff74-38a6-4f1a-bcce-e3f121b34fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_annotate(filename, model, foo, width_frame=1):\n",
    "    img = cv.imread(filename)\n",
    "    faces = foo(filename, model)\n",
    "    for (a, b, c, d) in faces:\n",
    "        cv.rectangle(img, (a, b), (c, d), (0, 0, 255), width_frame)\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-farming",
   "metadata": {},
   "source": [
    "## Detector 1 - OpenCV's Haar Cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "robust-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_detector = cv.CascadeClassifier('models/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "structural-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_haar_results(filename, detector):\n",
    "    img = cv.imread(filename)\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) \n",
    "    faces = detector.detectMultiScale(img_gray, scaleFactor = 1.2, \n",
    "                                      minNeighbors = 5)\n",
    "    faces = list(faces)\n",
    "    for face in faces:\n",
    "        face[2] += face[0]\n",
    "        face[3] += face[1]\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5096ca-b5c3-47ea-885a-c3dfe5ef53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                haar_detector,\n",
    "                                normalize_haar_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-modeling",
   "metadata": {},
   "source": [
    "## Detector 2 - OpenCV's DNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minus-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_detector = cv.dnn.readNetFromCaffe(\"models/deploy.prototxt.txt\",\n",
    "                                       \"models/res10_300x300_ssd_iter_140000.caffemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confirmed-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dnn_results(filename, model):\n",
    "    img = cv.imread(filename)\n",
    "    faces = []\n",
    "    h, w = img.shape[:2]\n",
    "    blob = cv.dnn.blobFromImage(cv.resize(img, (300, 300)),\n",
    "                                1.0, (300, 300), \n",
    "                                (104.0, 117.0, 123.0))\n",
    "    model.setInput(blob)\n",
    "    detected = model.forward()\n",
    "    for j in range(detected.shape[2]):\n",
    "        confidence = detected[0, 0, j, 2]\n",
    "        if confidence > 0.5:\n",
    "            box = detected[0, 0, j, 3:7] * np.array([w, h, w, h])\n",
    "            coords = box.astype(\"int\")\n",
    "            faces.append(coords)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                dnn_detector,\n",
    "                                normalize_dnn_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-mainstream",
   "metadata": {},
   "source": [
    "## Detector 3 - MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fitting-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "mtcnn_detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fiscal-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mtcnn_results(filename, model):\n",
    "    img = cv.imread(filename)\n",
    "    faces = []\n",
    "    boxes = model.detect_faces(img)\n",
    "    for box in boxes:\n",
    "        bdbox = box.get('box')\n",
    "        normalized_box = [bdbox[0], bdbox[1], bdbox[0]+bdbox[2], bdbox[1]+bdbox[3]]\n",
    "        faces.append(normalized_box)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                mtcnn_detector,\n",
    "                                normalize_mtcnn_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-testament",
   "metadata": {},
   "source": [
    "## Detector 4 - Dlib HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informative-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "hog_detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dress-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hog_results(filename, model, scale=2):\n",
    "    img = cv.imread(filename)\n",
    "    faces = []\n",
    "    boxes = model(img, scale)\n",
    "    for box in boxes:\n",
    "        faces.append([box.left(), box.top(), box.right(), box.bottom()])\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "harmful-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_annotate(img_path, model, width_frame=1, scale=2):\n",
    "    img = cv.imread(img_path)\n",
    "    faces = normalize_hog_results(img_path, model, scale)\n",
    "    for (a, b, c, d) in faces:\n",
    "        cv.rectangle(img, (a, b), (c, d), (0, 0, 255), width_frame)\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "\n",
    "changing_param = [1, 2, 4]\n",
    "for j in range(len(changing_param)):\n",
    "    for i in range(len(exemplary_imgs_files)):\n",
    "        image = hog_annotate(exemplary_imgs_files[i], hog_detector, scale=changing_param[j])\n",
    "        axes.append(fig.add_subplot(3, 3, 3*j+i+1))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-protein",
   "metadata": {},
   "source": [
    "## Detector 5 - dlib MMOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parliamentary-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "mmod_detector = dlib.cnn_face_detection_model_v1(\"models/mmod_human_face_detector.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ready-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mmod_results(filename, model):\n",
    "    img = cv.imread(filename)\n",
    "    faces = []\n",
    "    boxes = model(img, 2)\n",
    "    for box in boxes:\n",
    "        face = box.rect\n",
    "        faces.append([face.left(), face.top(), face.right(), face.bottom()])\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                mmod_detector,\n",
    "                                normalize_mmod_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-optimization",
   "metadata": {},
   "source": [
    "## Detector 6 - 3DDFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prepared-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "successful-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FaceBoxes = importlib.import_module('3DDFA_V2.FaceBoxes.FaceBoxes')\n",
    "mod_TDDFA = importlib.import_module('3DDFA_V2.TDDFA')\n",
    "TDDFA = getattr(mod_TDDFA, \"TDDFA\")\n",
    "mod_functions = importlib.import_module('3DDFA_V2.utils.functions')\n",
    "draw_landmarks = getattr(mod_functions, \"draw_landmarks\")\n",
    "mod_render = importlib.import_module('3DDFA_V2.utils.render')\n",
    "render = getattr(mod_render, \"render\")\n",
    "mod_depth = importlib.import_module('3DDFA_V2.utils.depth')\n",
    "depth = getattr(mod_depth, \"depth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "changed-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "cfg = yaml.load(open('3DDFA_V2/configs/mb1_120x120.yml'), Loader=yaml.SafeLoader)\n",
    "\n",
    "# Init FaceBoxes and TDDFA, recommend using onnx flag\n",
    "onnx_flag = True  # or True to use ONNX to speed up\n",
    "if onnx_flag:\n",
    "    import os\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "    os.environ['OMP_NUM_THREADS'] = '4'\n",
    "    \n",
    "    mod_FaceBoxesONNX = importlib.import_module('3DDFA_V2.FaceBoxes.FaceBoxes_ONNX')\n",
    "    FaceBoxes_ONNX = getattr(mod_FaceBoxesONNX, \"FaceBoxes_ONNX\")\n",
    "    mod_TDDFA_ONNX = importlib.import_module('3DDFA_V2.TDDFA_ONNX')\n",
    "    TDDFA_ONNX = getattr(mod_TDDFA_ONNX, \"TDDFA_ONNX\")\n",
    "    \n",
    "    face_boxes = FaceBoxes_ONNX()\n",
    "    tddfa = TDDFA_ONNX(**cfg)\n",
    "else:\n",
    "    tddfa = TDDFA(gpu_mode=False, **cfg)\n",
    "    face_boxes = FaceBoxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "designing-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_3ddfa_results(filename, model):\n",
    "    img = cv.imread(filename)\n",
    "    boxes = model(img)\n",
    "    faces = []\n",
    "    for box in boxes:\n",
    "        if box[4] > 0.5:\n",
    "            faces.append((int(box[0]), int(box[1]), int(box[2]), int(box[3])))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                face_boxes,\n",
    "                                normalize_3ddfa_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplary_imgs = [cv.imread(file) for file in exemplary_imgs_files]\n",
    "fig=plt.figure(figsize=(30, 15))\n",
    "axes = []\n",
    "dense_flag = False\n",
    "\n",
    "for i in range(len(exemplary_imgs)):\n",
    "    img = exemplary_imgs[i]\n",
    "    boxes = face_boxes(img)\n",
    "    param_lst, roi_box_lst = tddfa(img, boxes)\n",
    "    ver_lst = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=dense_flag)\n",
    "    draw_landmarks(img, ver_lst, dense_flag=dense_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-waters",
   "metadata": {},
   "source": [
    "## Detector 7 - CrowdHuman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd yolov4_crowdhuman/darknet\n",
    " ./darknet detector test data/crowdhuman-608x608.data \\\n",
    "                          cfg/yolov4-crowdhuman-608x608.cfg \\\n",
    "                          backup/yolov4-crowdhuman-608x608_best.weights \\\n",
    "                          -dont_show -ext_output < ../../dataset/trial/ground_truths.txt > ../../dataset/trial/ground_result.txt \\\n",
    "                          -out ../../dataset/trial/ground_result.json \\\n",
    "                          -gpus 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-insured",
   "metadata": {},
   "source": [
    "### Use CrowdHuman prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "three-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"dataset/trial/ground_result.json\") as file:\n",
    "    data_ground = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "separate-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_filenames = []\n",
    "for photo in data_ground:\n",
    "    ch_filenames.append(photo['filename'].replace(\"/am/dataset/second_approach/images/\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "joint-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_crowdhuman_results(filename, i=None):\n",
    "    img = cv.imread(filename)\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    faces  = []\n",
    "    file_name = filename.replace(\"dataset/second_approach/images/\", \"\")\n",
    "    idx = ch_filenames.index(file_name)\n",
    "    for detected in data_ground[idx][\"objects\"]:\n",
    "        if detected[\"name\"] == 'head' and detected[\"confidence\"] > 0.5 :\n",
    "            xmin = int((detected[\"relative_coordinates\"][\"center_x\"] - detected[\"relative_coordinates\"][\"width\"]/2)*width)\n",
    "            xmax = int((detected[\"relative_coordinates\"][\"center_x\"] + detected[\"relative_coordinates\"][\"width\"]/2)*width)\n",
    "            ymin = int((detected[\"relative_coordinates\"][\"center_y\"] - detected[\"relative_coordinates\"][\"height\"]/2)*height)\n",
    "            ymax = int((detected[\"relative_coordinates\"][\"center_y\"] + detected[\"relative_coordinates\"][\"height\"]/2)*height)\n",
    "            faces.append((xmin, ymin, xmax, ymax))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2d0a2-f980-462a-a636-3584e82dbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                None,\n",
    "                                normalize_crowdhuman_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-bradley",
   "metadata": {},
   "source": [
    "## Detector 8 - RetinaFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-obligation",
   "metadata": {},
   "source": [
    "It has landmarks - nose, mouth, eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "leading-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 16, 8] {'32': {'SCALES': (32, 16), 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}, '16': {'SCALES': (8, 4), 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}, '8': {'SCALES': (2, 1), 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}}\n",
      "use_landmarks True\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import insightface\n",
    "\n",
    "retina_detector = insightface.model_zoo.get_model('retinaface_r50_v1')\n",
    "retina_detector.prepare(ctx_id = -1, nms=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tired-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_retina_results(filename, model):\n",
    "    img = cv.imread(filename)\n",
    "    faces = []\n",
    "    bbox, landmark = model.detect(img, threshold=0.5, scale=1.0)\n",
    "    for (a, b, c, d, score) in bbox:\n",
    "        faces.append([int(a), int(b), int(c), int(d)])\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                retina_detector,\n",
    "                                normalize_retina_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-prime",
   "metadata": {},
   "source": [
    "## Detector 9 - CenterFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458fe62-fb90-494f-8915-3c132d10482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd models/centerface\n",
    "python demo.py '../../dataset/trial/ground_truths.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a244ac-e49b-4d69-95c5-0da7fc19190f",
   "metadata": {},
   "source": [
    "### Load CenterFace results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7060662f-ccdd-4adc-b902-af6192259b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"models/centerface/centerface_results.json\") as file:\n",
    "    centerface_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb013fb6-eed5-4af7-9986-5ceee3a0f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_filenames = []\n",
    "\n",
    "for i in range(len(centerface_results)):\n",
    "    photo = centerface_results[str(i)]\n",
    "    cf_filenames.append(photo['filename'].replace(\"/am/dataset/second_approach/images/\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97f35061-8895-41ac-9136-18d6608e0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_centerface_results(filename, i=None):\n",
    "    img = cv.imread(filename)\n",
    "    faces  = []\n",
    "    file_name = filename.replace(\"dataset/second_approach/images/\", \"\")\n",
    "    idx = cf_filenames.index(file_name)\n",
    "    file_results = centerface_results[str(idx)]\n",
    "    for i in range(len(file_results[\"objects\"])):\n",
    "        xmin = int(float((file_results[\"objects\"][str(i)][\"coordinates\"][\"x_min\"])))\n",
    "        xmax = int(float((file_results[\"objects\"][str(i)][\"coordinates\"][\"x_max\"])))\n",
    "        ymin = int(float((file_results[\"objects\"][str(i)][\"coordinates\"][\"y_min\"])))\n",
    "        ymax = int(float((file_results[\"objects\"][str(i)][\"coordinates\"][\"y_max\"])))\n",
    "        faces.append((xmin, ymin, xmax, ymax))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976cc76-3b84-47d2-ab2e-916e1d1ca72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                None,\n",
    "                                normalize_centerface_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-crown",
   "metadata": {},
   "source": [
    "## Detector 10 - Tina Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subsequent-tackle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/853 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "100%|██████████| 853/853 [04:44<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "CUDA_VISIBLE_DEVICES=\"0\" python vedadet/tools/infer.py vedadet/configs/infer/tinaface/tinaface.py dataset/trial/ground_truths.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eaffe01-c540-404d-b00d-690fb42eb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"vedadet/tina_results.json\") as file:\n",
    "    tina_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b35d3a32-8159-4b95-b3b7-9a12a4c6c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tina_filenames = []\n",
    "for i in range(len(tina_results)):\n",
    "    photo = tina_results[str(i)]\n",
    "    tina_filenames.append(photo['filename'].replace(\"/am/dataset/second_approach/images/\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ac8b283-ef81-4089-b81a-260117ef3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tina_results(filename, i=None):\n",
    "    img = cv.imread(filename)\n",
    "    faces  = []\n",
    "    file_name = filename.replace(\"dataset/second_approach/images/\", \"\")\n",
    "    idx = tina_filenames.index(file_name)\n",
    "    file_results = tina_results[str(idx)]\n",
    "    for i in range(len(file_results[\"objects\"])):\n",
    "        xmin = int(float((file_results[\"objects\"][str(i)][\"coordinates\"][\"x_min\"])))\n",
    "        xmax = int(float((file_results[\"objects\"][str(i)][\"coordinates\"][\"x_max\"])))\n",
    "        ymin = int(float((file_results[\"objects\"][str(i)][\"coordinates\"][\"y_min\"])))\n",
    "        ymax = int(float((file_results[\"objects\"][str(i)][\"coordinates\"][\"y_max\"])))\n",
    "        faces.append((xmin, ymin, xmax, ymax))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46cfbb-43d4-4751-bd3c-97fbc0764f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                None,\n",
    "                                normalize_tina_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc5258-a30a-4c8c-ae46-cbdf3b3082f0",
   "metadata": {},
   "source": [
    "## Detector 11 - Facenet pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd96d24b-04a6-4034-91a8-2f5f23933634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "facenet_detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4147732e-69a4-4a0b-be3d-10df102e77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_facenet_results(filename, model):\n",
    "    img = cv.imread(filename)\n",
    "    faces = []\n",
    "    boxes, probs = model.detect(img)\n",
    "    i = 0\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            if probs[i] > 0.5:\n",
    "                faces.append([int(coord) for coord in box])\n",
    "            i += 1\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e2ce2-50bd-4636-8b4a-8233b6470745",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                facenet_detector,\n",
    "                                normalize_facenet_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9025c0-3267-4411-93fc-2b1b3803ac0c",
   "metadata": {},
   "source": [
    "## Detector 12 - PyramidBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e66fda9-1f49-4dfb-8b38-2be3a92d068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OMP_NUM_THREADS set to 4, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.\n",
      "PLEASE USE OMP_NUM_THREADS WISELY.\n",
      "\u001b[33m[2021-05-01 19:06:34,135] [ WARNING]\u001b[0m - The _initialize method in HubModule will soon be deprecated, you can use the __init__() to handle the initialization of the object\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "\n",
    "pyramidbox_detector = hub.Module(name=\"pyramidbox_lite_server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf5dcb4b-cac6-46da-b526-9be75c80e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def normalize_pyramidbox_results(filename, model):\n",
    "    img = cv.imread(filename)\n",
    "    faces  = []\n",
    "    result = model.face_detection(images=[img])\n",
    "    for face in result[0][\"data\"]:\n",
    "        if face[\"confidence\"] > 0.5:\n",
    "            xmin = face[\"left\"]\n",
    "            xmax = face[\"right\"]\n",
    "            ymin = face[\"top\"]\n",
    "            ymax = face[\"bottom\"]\n",
    "            faces.append((xmin, ymin, xmax, ymax))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078958b-48c1-4e45-a95d-22f3d7442611",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 20))\n",
    "axes = []\n",
    "for i in range(len(exemplary_imgs_files)):\n",
    "    image = detect_and_annotate(exemplary_imgs_files[i],\n",
    "                                pyramidbox_detector,\n",
    "                                normalize_pyramidbox_results)\n",
    "    axes.append(fig.add_subplot(1, 3, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-creek",
   "metadata": {},
   "source": [
    "## Detector comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bridal-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_pass(bb_target, bb_pred):\n",
    "    x_min = max(bb_target[0], bb_pred[0]) \n",
    "    y_min = max(bb_target[1], bb_pred[1])\n",
    "    x_max = min(bb_target[2], bb_pred[2]) \n",
    "    y_max = min(bb_target[3], bb_pred[3]) \n",
    "    inter = max(0, x_max-x_min+1)*max(0, y_max-y_min+1)\n",
    "    union = (bb_pred[2]-bb_pred[0]+1)*(bb_pred[3]-bb_pred[1]+1) + (bb_target[2]-bb_target[0]+1)*(bb_target[3]-bb_target[1]+1) - inter\n",
    "    if inter/union > 0.5:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hazardous-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_class(area):\n",
    "    if area < 16:\n",
    "        return -1\n",
    "    if area < 32**2:\n",
    "        return 0\n",
    "    if area < 96**2:\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "experienced-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_prediction(bdbox, width, height, faces, unused, not_in_iou):\n",
    "    is_left = True\n",
    "    xmin = int((int(bdbox.find('xmin').text))/width)\n",
    "    ymin = int((int(bdbox.find('ymin').text))/height)\n",
    "    xmax = int((int(bdbox.find('xmax').text))/width)\n",
    "    ymax = int((int(bdbox.find('ymax').text))/height)\n",
    "    size_group = size_class((ymax-ymin)*(xmax-xmin))\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    if size_group != -1:\n",
    "        for j in range(len(faces)):\n",
    "            if iou_pass((xmin, ymin, xmax, ymax), (faces[j][0], faces[j][1], faces[j][2], faces[j][3])):\n",
    "                if (not is_left or not unused[j]):\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "                    is_left = False\n",
    "                    unused[j] = False\n",
    "                not_in_iou[j] = False\n",
    "        if (is_left):\n",
    "            FN = 1\n",
    "    return size_group, TP, FN, FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "liked-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detections(model, norm_res, annots):\n",
    "    FP = np.zeros(( 3, len(annots)))\n",
    "    TP = np.zeros((3, len(annots)))\n",
    "    FN = np.zeros((3, len(annots)))\n",
    "    k = 0\n",
    "    for img_annot in annots:\n",
    "        tree = ET.parse(img_annot)\n",
    "        root = tree.getroot()\n",
    "    \n",
    "        file = root.find('filename').text\n",
    "        file_path = img_dir + file\n",
    "        img = cv.imread(file_path)\n",
    "        objects = root.findall('object')\n",
    "        size = root.find('size')\n",
    "        width = img.shape[1]/int(size.find('width').text)\n",
    "        height = img.shape[0]/int(size.find('height').text)\n",
    "        faces = norm_res(file_path, model)\n",
    "        unused = [True]*len(faces)\n",
    "        not_in_iou = [True]*len(faces)\n",
    "        for i in range(len(objects)):\n",
    "            bndbox = objects[i].find('bndbox')\n",
    "            size_group, tp, fn, fp = estimate_model_prediction(bndbox, width, height, faces, unused, not_in_iou)\n",
    "            TP[size_group, k] += tp\n",
    "            FN[size_group, k] += fn\n",
    "            FP[size_group, k] += fp\n",
    "        FP[size_group, k] += sum(not_in_iou)\n",
    "        k += 1 \n",
    "    return FP, TP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ideal-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detection_results(FP, TP, FN, model_name, filename='results.csv', header=False, mode='a'):\n",
    "    overdetected = []\n",
    "    for i in range(len(size)):\n",
    "        overdetected.append(np.sum(FP[i, :]))\n",
    "    d = pd.DataFrame(np.reshape(overdetected, (1, len(size))))\n",
    "    d.index = [model_name]\n",
    "    d.columns = [\"over_\"+cat for cat in size]\n",
    "\n",
    "    for i in range(len(size)):\n",
    "        det = []\n",
    "        under = []\n",
    "        det.append(np.sum(TP[i, :]))\n",
    "        under.append(np.sum(FN[i, :]))\n",
    "        d[\"det_\"+size[i]] = det\n",
    "        d[\"under_\"+size[i]] = under\n",
    "        d[\"total_faces_\"+size[i]] = d[\"under_\"+size[i]] +  d[\"det_\"+size[i]]\n",
    "        d[\"total_\"+size[i]] = d[\"total_faces_\"+size[i]] + d[\"over_\"+size[i]]\n",
    "    d = d[sorted(d.columns.values, key = lambda x:x[-1])]\n",
    "    d.to_csv(filename, header=header, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "residential-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [\"S\", \"M\", \"L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dd40c66-6f45-494b-b56c-459e6e632e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Haar\", \"DNN\", \"MTCNN\", \"HOG\", \"MMOD\", \n",
    "         \"3DDFA\", \"CrowdHuman\", \"RetinaFace\",\n",
    "        \"CenterFace\", \"TinaFace\", \"Facenet\", \"PyramidBox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29cdcbd5-059f-451e-9e5a-5e591aa5b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_precision_and_recall(FP, TP, FN, model_name, filename='results.csv', header=False, mode='a'):\n",
    "    FP_img = np.sum(FP, axis=0)\n",
    "    TP_img = np.sum(TP, axis=0)\n",
    "    FN_img = np.sum(FN, axis=0)\n",
    "    Precision = np.divide(TP_img, (FP_img+TP_img), out=np.zeros(TP_img.shape, dtype=float), where=(FP_img+TP_img)!=0)\n",
    "    Recall = TP_img / (TP_img+FN_img)\n",
    "    d = pd.DataFrame(data=[Precision, Recall], index=[model_name+\"_P\", model_name+\"_R\"])\n",
    "    d.to_csv(filename, header=header, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-thumbnail",
   "metadata": {},
   "source": [
    "### Haar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "developmental-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(haar_detector, normalize_haar_results, annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "improved-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_detection_results(FP, TP, FN, names[0], filename='results.csv', header=True, mode='w')\n",
    "save_precision_and_recall(FP, TP, FN, names[0], filename='PR.csv', header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "relevant-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "del FP, TP, FN, haar_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-cycling",
   "metadata": {},
   "source": [
    "### DNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e89f6c4-9d05-4408-982e-adac0f52830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(dnn_detector, normalize_dnn_results, annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cec28de-6837-4550-b0ed-b78d0419cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_detection_results(FP, TP, FN, names[1], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[1], filename='PR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e267469f-5c6f-4fd4-b25b-51ecd6be21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del FP, TP, FN, dnn_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69ea9d-b670-4aa4-b531-dccc932d7c60",
   "metadata": {},
   "source": [
    "### MTCNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f01fb443-1cd2-46b8-aeba-7e185f3e6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(mtcnn_detector, normalize_mtcnn_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[2], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[2], filename='PR.csv')\n",
    "del FP, TP, FN, mtcnn_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3937e-ead6-4bca-a6aa-1c12d7cba47f",
   "metadata": {},
   "source": [
    "### HOG results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39981c6e-5f05-4e8c-a47b-841e83406557",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(hog_detector, normalize_hog_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[3], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[3], filename='PR.csv')\n",
    "del FP, TP, FN, hog_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfe084-323d-4ec4-8e7e-c8306168b02d",
   "metadata": {},
   "source": [
    "### MMOD results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86c66651-3cc1-4f60-87b0-ccbd05d8e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(mmod_detector, normalize_mmod_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[4], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[4], filename='PR.csv')\n",
    "del FP, TP, FN, mmod_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3fffd-e792-42d7-ba14-871da4b280c4",
   "metadata": {},
   "source": [
    "### 3DDFA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1bac7b8-46cc-4a77-b267-34922aa0a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(face_boxes, normalize_3ddfa_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[5], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[5], filename='PR.csv')\n",
    "del FP, TP, FN, face_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119d0f5-8f15-46f8-90af-9c46ce90a09f",
   "metadata": {},
   "source": [
    "### CrowdHuman results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aebe91c1-d3f7-4aac-b389-9fd904b41d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(None, normalize_crowdhuman_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[6], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[6], filename='PR.csv')\n",
    "del FP, TP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91f52e-f286-4640-adc6-17ca7295212d",
   "metadata": {},
   "source": [
    "### Retina Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30fbfa55-571f-4f6d-8385-ef03d5585a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(retina_detector, normalize_retina_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[7], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[7], filename='PR.csv')\n",
    "del FP, TP, FN, retina_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d8cb4-24a8-4184-881f-1d8194a8456a",
   "metadata": {},
   "source": [
    "### CenterFace Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "broad-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(None, normalize_centerface_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[8], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[8], filename='PR.csv')\n",
    "del FP, TP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9fa267-33c6-4003-8b57-e39260e8c971",
   "metadata": {},
   "source": [
    "### Tina results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba9907c3-f710-4fc9-995c-364a9f4512cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(None, normalize_tina_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[9], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[9], filename='PR.csv')\n",
    "del FP, TP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e45ab-1ed2-40ca-b204-b1a5a37fa065",
   "metadata": {},
   "source": [
    "### Facenet pytorch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5df96083-37fc-4c16-aa73-617d0da45b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(facenet_detector, normalize_facenet_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[10], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[10], filename='PR.csv')\n",
    "del FP, TP, FN, facenet_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22663a99-91e5-4462-8229-1ea4eda82341",
   "metadata": {},
   "source": [
    "### Paddle Paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fffb0c5-5d3c-4be1-8fc0-400c5eeb97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP, TP, FN = get_detections(pyramidbox_detector, normalize_pyramidbox_results, annot)\n",
    "save_detection_results(FP, TP, FN, names[11], filename='results.csv')\n",
    "save_precision_and_recall(FP, TP, FN, names[11], filename='PR.csv')\n",
    "del FP, TP, FN, pyramidbox_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-interstate",
   "metadata": {},
   "source": [
    "### Comparison on face sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "after-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "serial-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_names = [\"Small\", \"Medium\", \"Large\"]\n",
    "size = [\"S\", \"M\", \"L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526a581e-60a2-49bd-a88f-989452b28cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv(\"results.csv\")\n",
    "res.rename(columns={'Unnamed: 0': 'index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b72dd09-9e19-44a4-a791-91f2c9f43dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = pd.read_csv(\"PR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f49c19e-1635-4609-aa54-8cc007be74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.rename(columns={'Unnamed: 0': 'index'}, inplace=True)\n",
    "pr.index = pr[\"index\"]\n",
    "pr.drop(columns=[\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30b4c6-85ea-4930-a760-a68b4f55b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = []\n",
    "for name in names:\n",
    "    precision = pr.loc[name+\"_P\"]\n",
    "    recall= pr.loc[name+\"_R\"]\n",
    "    recall, precision = (list(t) for t in zip(*sorted(zip(recall, precision))))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(recall, precision, color=\"red\")\n",
    "    plt.xlabel(\"Recall\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Precision\", fontsize=12, fontweight='bold')\n",
    "    plt.title(name, fontsize=15, fontweight=\"bold\")\n",
    "    precision = np.array(precision)\n",
    "    recall = np.array(recall)\n",
    "    AP = np.sum((recall[:-1] - recall[1:]) * precision[:-1])\n",
    "    ap.append(AP)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3c02f0e-243d-4282-b74e-30ed048e276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df = pd.DataFrame(ap, index=names)\n",
    "ap_df.columns = ['AP']\n",
    "ap_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4c13577-b546-4e0e-b074-77edcb3c5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.merge(ap_df, on=\"index\", how = 'inner')\n",
    "res  = res.sort_values(by=['AP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a1bc4e9-c409-4ce5-9f1a-3a67b280c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sizes_names)):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    bar1 = sns.barplot(x=\"index\",  y=\"total_\"+size[i], data=res, color=sns.color_palette(\"muted\")[1])\n",
    "    bar2 = sns.barplot(x=\"index\", y=\"total_faces_\"+size[i], data=res,  color=sns.color_palette(\"muted\")[3])\n",
    "    bar3 = sns.barplot(x=\"index\", y=\"det_\"+size[i], data=res,  color=sns.color_palette(\"muted\")[2])\n",
    "    plt.title(sizes_names[i] +\" faces\", size=18)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"Occurences\")\n",
    "    top_bar = mpatches.Patch(color=sns.color_palette(\"muted\")[2], label='Detected')\n",
    "    bottom_bar = mpatches.Patch(color=sns.color_palette(\"muted\")[3], label='Underdetected')\n",
    "    medium_bar = mpatches.Patch(color=sns.color_palette(\"muted\")[1], label='Overdetected')\n",
    "    plt.legend(handles=[top_bar, bottom_bar, medium_bar], bbox_to_anchor=(1.05, 0.5), \n",
    "           loc='center left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-trash",
   "metadata": {},
   "source": [
    "### Best and worst photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48463ea3-3510-48a0-b875-05ed25a85216",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = pd.read_csv(\"PR.csv\")\n",
    "pr.index = pr[\"index\"]\n",
    "pr.drop(columns=[\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e474cf-8dff-4956-a101-83c96429882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_idx = int(pr.sum(axis=0).idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c560b4aa-9a30-4cae-821a-9e85d5305844",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = int(pr.sum(axis=0).idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dfe27f6-d390-4405-ad36-ceb007f04087",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_annot = [annot[worst_idx],annot[best_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "signal-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_imgs = [(file.replace(\".xml\", \".png\")).replace(\"annotations\", \"images\") for file in wb_annot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80f48f8d-6a06-4af8-8423-5a941800fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Haar\", \"DNN\", \"MTCNN\", \"Facenet\",\n",
    "         \"HOG\", \"MMOD\", \"3DDFA\", \"CrowdHuman\", \n",
    "         \"RetinaFace\", \"CenterFace\", \"TinaFace\", \"PyramidBox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "selected-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "foos = [(normalize_haar_results, haar_detector), (normalize_dnn_results, dnn_detector), \n",
    "        (normalize_mtcnn_results, mtcnn_detector), (normalize_facenet_results, facenet_detector),\n",
    "        (normalize_hog_results, hog_detector), (normalize_mmod_results, mmod_detector),\n",
    "        (normalize_3ddfa_results, face_boxes), (normalize_crowdhuman_results, None),\n",
    "        (normalize_retina_results, retina_detector), (normalize_centerface_results, None), \n",
    "        (normalize_tina_results, None), (normalize_pyramidbox_results, pyramidbox_detector)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(wb_imgs_)):\n",
    "    fig=plt.figure(figsize=(20, 20))\n",
    "    axes = []\n",
    "    for j in range(len(foos)):\n",
    "        image = detect_and_annotate(wb_imgs[i], foos[j][1], foos[j][0], width_frame=1)\n",
    "        axes.append(fig.add_subplot(3, 4, j+1))\n",
    "        plt.text(0, 0, names[j], fontsize=30)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
